{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.classify import ClassifierI\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              tweet\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0  @Kenichan I dived many times for the ball. Man...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0  @nationwideclass no, it's not behaving at all....\n",
       "5       0                      @Kwesidei not the whole crew \n",
       "6       0                                        Need a hug \n",
       "7       0  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
       "8       0               @Tatiana_K nope they didn't have it \n",
       "9       0                          @twittera que me muera ? "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./input_data1/tweet_data.csv\",names = [\"target\",\"tweet no\",\"date\",\"---\",\"user\",\"tweet\"],encoding=\"latin-1\")\n",
    "data = data.drop(columns=[\"tweet no\",\"date\",\"---\",\"user\"])\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_tweets = data[data.target == 4]\n",
    "# negative_tweets = data[data.target == 0]\n",
    "\n",
    "# stop_words = list(set(stopwords.words('english')))\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# all_words = []\n",
    "# documents = []\n",
    "\n",
    "# for p in positive_tweets[\"tweet\"]:\n",
    "    \n",
    "#     # create a list of tuples where the first element of each tuple is a review\n",
    "#     # the second element is the label\n",
    "#     documents.append( (p, \"pos\") )\n",
    "    \n",
    "#     # remove punctuations\n",
    "#     cleaned = re.sub(r'[^(a-zA-Z)\\s]','', p)\n",
    "    \n",
    "#     # tokenize \n",
    "#     tokenized = word_tokenize(cleaned)\n",
    "    \n",
    "#     # remove stopwords \n",
    "#     tweet = [w for w in tokenized if not w in stop_words]\n",
    "    \n",
    "#     ps = PorterStemmer()\n",
    "#     tweet = [ ps.stem(word) for word in tweet ]\n",
    "#     print (tokenized)\n",
    "    \n",
    "# for p in negative_tweets[\"tweet\"]:\n",
    "#         documents.append( (p, \"neg\") )\n",
    "    \n",
    "#     # remove punctuations\n",
    "#         cleaned = re.sub(r'[^(a-zA-Z)\\s]','', p)\n",
    "    \n",
    "#     # tokenize \n",
    "#         tokenized = word_tokenize(cleaned)\n",
    "    \n",
    "#     # remove stopwords \n",
    "#         stopped = [w for w in tokenized if not w in stop_words]\n",
    "#         print (tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/niharika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/niharika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/niharika/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "tweet_pos = data[data.target == 4]\n",
    "tweet_neg = data[data.target == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in  tweet_pos['tweet']:\n",
    "    #remove @username\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    \n",
    "    # Remove tickers\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # To lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove hyperlinks starting with http*\n",
    "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*','', tweet)\n",
    "    \n",
    "    # Remove hyperlinks starting with www.?*\n",
    "    tweet = re.sub(r'www.[^ ]+','', tweet)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r'#\\w*', '', tweet)\n",
    "    \n",
    "    # remove non ascii\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]',' ', tweet)\n",
    "    \n",
    "    # Remove Punctuation and split 's, 't, 've with a space for filter\n",
    "    tweet = re.sub(r'[' + string.punctuation.replace('@', '') + ']+', ' ', tweet)\n",
    "\n",
    "    # remove anything that is not alphanumeric\n",
    "    tweet = re.sub('[\\W_]+', ' ', tweet)\n",
    "    \n",
    "    # tokenize \n",
    "    tweet = word_tokenize(tweet)\n",
    "    \n",
    "    # remove stopwords \n",
    "    tweet = [w for w in tweet if not w in stop_words]\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tweet = [ ps.stem(word) for word in tweet ] \n",
    "    for w in tweet:\n",
    "        if w not in all_words:\n",
    "            all_words.append(w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in  tweet_neg['tweet']:\n",
    "    #remove @username\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    \n",
    "    # Remove tickers\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # To lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove hyperlinks starting with http*\n",
    "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*','', tweet)\n",
    "    \n",
    "    # Remove hyperlinks starting with www.?*\n",
    "    tweet = re.sub(r'www.[^ ]+','', tweet)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r'#\\w*', '', tweet)\n",
    "    \n",
    "    # remove non ascii\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]',' ', tweet)\n",
    "    \n",
    "    # Remove Punctuation and split 's, 't, 've with a space for filter\n",
    "    tweet = re.sub(r'[' + string.punctuation.replace('@', '') + ']+', ' ', tweet)\n",
    "\n",
    "    # remove anything that is not alphanumeric\n",
    "    tweet = re.sub('[\\W_]+', ' ', tweet)\n",
    "    \n",
    "    # tokenize \n",
    "    tweet = word_tokenize(tweet)\n",
    "    \n",
    "    # remove stopwords \n",
    "    tweet = [w for w in tweet if not w in stop_words]\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tweet = [ ps.stem(word) for word in tweet ] \n",
    "    for w in tweet:\n",
    "        if w not in all_words:\n",
    "            all_words.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(word_features,document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tuple = []\n",
    "\n",
    "for tweet in tweet_pos['tweet']:\n",
    "    tweet_tuple.append( (tweet, \"pos\") )\n",
    "    \n",
    "for tweet in tweet_neg['tweet']:\n",
    "    tweet_tuple.append( (tweet, \"neg\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 100.0\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK RESULTS ON TWEETS: )\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "num_features = range(10000,100001,10000):\n",
    "for n in num_features:\n",
    "    word_features = list(all_words.keys())[:n]\n",
    "    train_val_test = [(find_features(word_features,tweet), target) for (tweet, target) in tweet_tuple]\n",
    "    \n",
    "    t1 = int(len(train_val_test)*0.98)\n",
    "    t2 = int(len(train_val_test)*0.99)\n",
    "    train = train_val[:t1]\n",
    "    val = train_val[t1:t2]\n",
    "    test = train_val[t2:]\n",
    "    \n",
    "    t0 = time()\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "    train_time = time() - t0\n",
    "    \n",
    "    print(\"Time to train:\",train_time)\n",
    "    print(\"Validation accuracy percent:\",(nltk.classify.accuracy(classifier, val))*100)\n",
    "    print(\"Test accuracy percent:\",(nltk.classify.accuracy(classifier, test))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
